{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3ff56c7",
      "metadata": {},
      "source": [
        "# Langchain bases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0fa61a",
      "metadata": {},
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410d243c",
      "metadata": {},
      "source": [
        "- Setup model, \n",
        "- Langchain resources, \n",
        "- LCEL (Langchain Expression Language) \n",
        "- Prompt template\n",
        "- Output parser\n",
        "- Parallelism (chains)\n",
        "- Quick RAG search example\n",
        "- Tools \n",
        "- Agents\n",
        "- Agents type and agents events \n",
        "- Raising an agent\n",
        "- Memory "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5362d239",
      "metadata": {},
      "source": [
        "# Setup Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c745b57",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-core langchain-community langchain-openai langchain docarray tiktoken langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7b93d0e8",
      "metadata": {
        "height": 115,
        "id": "7b93d0e8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.tools import tool\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
        "from langchain_core.messages import HumanMessage, AIMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25a2fed",
      "metadata": {},
      "source": [
        "# Setup model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78c92400",
      "metadata": {
        "id": "78c92400"
      },
      "outputs": [],
      "source": [
        "with open('../credentials.json') as jsonfile:\n",
        "    credentials = json.load(jsonfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6edca54f",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('../embeddings.json') as jsonfile:\n",
        "    embeddings = json.load(jsonfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f2adf1f",
      "metadata": {
        "id": "4f2adf1f"
      },
      "outputs": [],
      "source": [
        "model = AzureChatOpenAI(\n",
        "                        azure_endpoint = credentials['OpenAI']['base'],\n",
        "                        openai_api_version = credentials['OpenAI']['version'],\n",
        "                        openai_api_key = credentials['OpenAI']['key'],\n",
        "                        openai_api_type = credentials['OpenAI']['type'],\n",
        "                        deployment_name = credentials['OpenAI']['deployment_name'],\n",
        "                        temperature = 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "991166da",
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = AzureOpenAIEmbeddings(\n",
        "                        azure_endpoint = embeddings['embeddings']['base'],\n",
        "                        openai_api_version = embeddings['embeddings']['version'],\n",
        "                        openai_api_key = embeddings['embeddings']['key'],\n",
        "                        openai_api_type = embeddings['embeddings']['type'],\n",
        "                        azure_deployment = embeddings['embeddings']['deployment_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b204e360",
      "metadata": {},
      "source": [
        "# Langchain resources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffdb7f8",
      "metadata": {},
      "source": [
        "## Currently\n",
        "\n",
        "- ``langchain-core``: Base abstractions and LangChain Expression Language.\n",
        "- ``langchain-community``: Third party integrations.\n",
        "    - Partner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\n",
        "- ``langchain``: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
        "- ``langgraph``: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n",
        "- ``langserve``: Deploy LangChain chains as REST APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4a1db5",
      "metadata": {},
      "source": [
        "![](https://python.langchain.com/svg/langchain_stack.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b62e2b",
      "metadata": {
        "id": "46b62e2b"
      },
      "source": [
        "# How langchain works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "348a2991",
      "metadata": {
        "id": "348a2991"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/davidmigloz/langchain_dart/main/docs/img/langchain.dart.png\n",
        "\" width=\"1000\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9e80cb",
      "metadata": {},
      "source": [
        "# How prompt templates work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517b13e2",
      "metadata": {},
      "source": [
        "## LCEL: Basic example: prompt + model + output parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bc188d",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd58af87",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain.invoke({\"topic\": \"ice cream\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886deba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://python.langchain.com/docs/modules/model_io/output_parsers/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb0517d",
      "metadata": {},
      "source": [
        "## Prompt template + tools testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fa4a4fbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set a prompt template \n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "system = prompt_template.format(adjective=\"funny\", content=\"chickens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "13fc5888",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def chain_testing_prompt_tools(input_text) -> str:\n",
        "    \"\"\"\n",
        "    Chain to tell a very funny joke\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system),\n",
        "            (\"user\", \"{input}\")\n",
        "        ])\n",
        "\n",
        "    chain = prompt | model \n",
        "\n",
        "    return chain.invoke({\"input\": input_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee0853d",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain_testing_prompt_tools.invoke(\"Tell me a joke about chickens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e51938f",
      "metadata": {},
      "source": [
        "## Pydantic + output parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1f9e28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.pydantic.dev/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba280f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define your desired data structure.\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "    # you can add custom validation logic easily with Pydantic.\n",
        "    @validator(\"setup\")\n",
        "    def question_ends_with_question_mark(cls, field):\n",
        "        if field[-1] != \"?\":\n",
        "            raise ValueError(\"Badly formed question!\")\n",
        "        return field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ced2668e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=Joke)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3605966b",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9d2d460",
      "metadata": {},
      "outputs": [],
      "source": [
        "# and a query intended to prompt a language model to populate the data structur\n",
        "\n",
        "prompt_and_model = prompt | model\n",
        "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
        "parser.invoke(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb768a93",
      "metadata": {},
      "source": [
        "## Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc40afc",
      "metadata": {},
      "outputs": [],
      "source": [
        "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
        "poem_chain = (ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f917fa4",
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_chains = RunnableParallel(joke=joke_chain,bears=poem_chain)\n",
        "\n",
        "# we can invoke the runnable normally using the invoke method\n",
        "combined_chains.invoke({\"topic\":\"bears\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091ef40e",
      "metadata": {},
      "source": [
        "### Parallelism can also be used with batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aead404e",
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_chains.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85e5a52",
      "metadata": {},
      "source": [
        "# RAG Search Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca04025",
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1176d5fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9c3fc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec3b8a62",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b1f0a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = setup_and_retrieval | prompt | model | output_parser\n",
        "\n",
        "chain.invoke(\"where did harrison work?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1f7754",
      "metadata": {},
      "source": [
        "# How Langchain tools work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "376028e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a tool using the '@tool' decorator. \n",
        "# this tool will perform a multiplication operation.\n",
        "@tool\n",
        "def multiply(first_int: int, second_int: int) -> int:\n",
        "    \"\"\"\n",
        "    This function multiplies two integers together and returns the result.\n",
        "    \n",
        "    Parameters:\n",
        "    first_int (int): The first integer to be multiplied.\n",
        "    second_int (int): The second integer to be multiplied.\n",
        "\n",
        "    Returns:\n",
        "    int: The product of the two input integers.\n",
        "    \"\"\"\n",
        "    # perform the multiplication operation and return the result.\n",
        "    return first_int * second_int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d6593e",
      "metadata": {},
      "source": [
        "- the '@tool' decorator is used to indicate that you want to create your own tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a2ec2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9faaa6c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "multiply.invoke({\"first_int\": 4, \"second_int\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38d0f81",
      "metadata": {
        "id": "f38d0f81"
      },
      "source": [
        "# How Langchain agents work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64619f90",
      "metadata": {
        "id": "64619f90"
      },
      "source": [
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:1358/1*5TnpUZnp4-sq8TuJGYe_-w.png\" width=\"1300\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b28fff",
      "metadata": {
        "id": "66b28fff"
      },
      "source": [
        "- Agents are a crucial component of Langchain. \n",
        "- They are tasked with making decisions and carrying out actions based on a language model. \n",
        "- Unlike action chains, where a sequence of actions is directly hardcoded into the code, agents employ a language model as a reasoning mechanism to determine which actions to take and in what order."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3b9ad2",
      "metadata": {
        "id": "0e3b9ad2"
      },
      "source": [
        "## Built-in LangChain tools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a1853b",
      "metadata": {
        "id": "93a1853b"
      },
      "source": [
        "## Agent types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968c44c4",
      "metadata": {
        "id": "968c44c4"
      },
      "source": [
        "- ``CHAT_CONVERSATIONAL_REACT_DESCRIPTION``: Este agente é projetado para ser usado em configurações de conversação. Ele usa o framework ReAct para decidir qual ferramenta usar e usa a memória para lembrar as interações anteriores da conversa.\n",
        "\n",
        "- ``CHAT_ZERO_SHOT_REACT_DESCRIPTION``: Este agente é uma versão otimizada para modelos de chat. Ele usa o framework ReAct para decidir qual ferramenta usar e pode invocar ferramentas com várias entradas.\n",
        "\n",
        "- ``CONVERSATIONAL_REACT_DESCRIPTION``: Este agente usa o framework ReAct para decidir qual ferramenta usar com base na descrição da ferramenta. Ele pode receber várias ferramentas como entrada.\n",
        "\n",
        "- ``OPENAI_FUNCTIONS``: Este agente é otimizado para usar funções específicas do OpenAI. Ele é projetado para trabalhar com modelos que foram ajustados para detectar quando uma função deve ser chamada e responder com as entradas que devem ser passadas para a função.\n",
        "\n",
        "- ``OPENAI_MULTI_FUNCTIONS``: Este agente é uma versão estendida do OPENAI_FUNCTIONS e suporta a chamada de várias funções ao mesmo tempo. Isso pode acelerar a execução de agentes em certas tarefas.\n",
        "\n",
        "- ``REACT_DOCSTORE``: Este agente usa o framework ReAct para interagir com um repositório de documentos. Ele requer duas ferramentas: uma ferramenta de pesquisa para procurar um documento e uma ferramenta de consulta para procurar um termo no documento mais recente encontrado.\n",
        "\n",
        "- ``SELF_ASK_WITH_SEARCH``: Este agente usa uma única ferramenta chamada \"Intermediate Answer\" para procurar respostas factuais para perguntas. Ele é equivalente ao artigo original \"self-ask with search\", onde uma API de pesquisa do Google foi usada como ferramenta.\n",
        "\n",
        "- ``STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION``: Este agente é otimizado para modelos de chat e pode usar ferramentas com várias entradas. Ele usa o framework ReAct e é capaz de criar uma entrada de ação estruturada com base no esquema de argumentos da ferramenta.\n",
        "\n",
        "- ``ZERO_SHOT_REACT_DESCRIPTION``: Este agente usa o framework ReAct para decidir qual ferramenta usar com base na descrição da ferramenta. Ele pode receber várias ferramentas como entrada e é o agente mais geral e versátil."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048624cd",
      "metadata": {
        "id": "048624cd"
      },
      "source": [
        "## Agent events"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6888f45",
      "metadata": {
        "id": "a6888f45"
      },
      "source": [
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*uEAfllPdUxZKEkiRIuZFdA.png\" width=\"1500\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7d3bf1",
      "metadata": {
        "id": "fd7d3bf1"
      },
      "source": [
        "- Considerando o diagrama abaixo, ao receber uma solicitação _(task)_, os Agentes aproveitam os LLM's para tomar uma decisão sobre qual ação tomar.\n",
        "- Após a conclusão de uma _ação_ (action), o Agente entra na etapa de _observação_ (observation).\n",
        "- Na etapa de _observação_, o Agente compartilha um _pensamento_ (thought), se uma resposta final não for alcançada,\n",
        "- O Agente volta para outra _ação_ para se aproximar de uma _resposta final_.\n",
        "- É possível _setar_ a quantidade máxima de iterações que o Agent pode recorrer para encontrar a _resposta final_ (max_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b132a9",
      "metadata": {},
      "source": [
        "# Build agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa59aa1",
      "metadata": {},
      "source": [
        "### Create custom tools from functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98380540",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def multiply(first_int: int, second_int: int) -> int:\n",
        "    \"\"\"\n",
        "    Multiply two integers together and return the result.\n",
        "\n",
        "    Parameters:\n",
        "    first_int (int): The first integer to be multiplied.\n",
        "    second_int (int): The second integer to be multiplied.\n",
        "\n",
        "    Returns:\n",
        "    int: The product of the two input integers.\n",
        "    \"\"\"\n",
        "    return first_int * second_int\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(first_int: int, second_int: int) -> int:\n",
        "    \"\"\"\n",
        "    Add two integers together and return the result.\n",
        "\n",
        "    Parameters:\n",
        "    first_int (int): The first integer to be added.\n",
        "    second_int (int): The second integer to be added.\n",
        "\n",
        "    Returns:\n",
        "    int: The sum of the two input integers.\n",
        "    \"\"\"\n",
        "    return first_int + second_int\n",
        "\n",
        "\n",
        "@tool\n",
        "def exponentiate(base: int, exponent: int) -> int:\n",
        "    \"\"\"\n",
        "    Raise the base to the power of the exponent and return the result.\n",
        "\n",
        "    Parameters:\n",
        "    base (int): The base number.\n",
        "    exponent (int): The exponent to which the base is raised.\n",
        "\n",
        "    Returns:\n",
        "    int: The result of raising the base to the power of the exponent.\n",
        "    \"\"\"\n",
        "    return base**exponent\n",
        "\n",
        "# define a list of tools for further use\n",
        "tools = [multiply, add, exponentiate]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c388a17d",
      "metadata": {},
      "source": [
        "### Create prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41e28b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the prompt to use\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt.pretty_print()\n",
        "\n",
        "\n",
        "# read more about langchain hub: https://docs.smith.langchain.com/cookbook/hub-examples "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88fa5d01",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = create_openai_tools_agent(model, tools, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34b3b34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e8ac1f",
      "metadata": {},
      "source": [
        "- o ``AgentExecutor`` é uma classe na LangChain que atua como um agente que utiliza ferramentas para executar tarefas. \n",
        "- ele é responsável por criar um modelo válido a partir de dados de entrada, lidar com paradas precoces, e executar ações com base em um plano determinado pelo agente. \n",
        "- o ``AgentExecutor`` também suporta callbacks para manipular eventos durante a execução da cadeia. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882bb80e",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# pegue 3 elevado à quinta potência\n",
        "# multiplique pela soma de doze e três\n",
        "# eleve ao quadrado o resultado total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbfee04",
      "metadata": {},
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17106823",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d44856f",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"hi!\")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(\"whats up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ca7635",
      "metadata": {},
      "source": [
        "- ``ChatMessageHistory()``: é uma classe na LangChain que serve como um invólucro leve para salvar mensagens de usuário e de IA, além de recuperá-las. \n",
        "    - pode ser usada diretamente para gerenciar a memória fora de uma cadeia\n",
        "\n",
        "- ``add_user_message(message)`` e ``add_ai_message(message)``: são métodos da classe ``ChatMessageHistory()`` que permitem adicionar mensagens de usuário e de IA, respectivamente, à memória. \n",
        "    - eles são métodos de conveniência para adicionar mensagens humanas e de IA à memória. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1af1fa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_ephemeral_chat_history.messages\n",
        "\n",
        "# see all msgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e68f6d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_ephemeral_chat_history.add_user_message(\n",
        "    \"Translate this sentence from English to French: I love programming.\"\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89be7463",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate this sentence from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d118ac",
      "metadata": {},
      "source": [
        "# Refs\n",
        "## Langchain docs \n",
        "- [LCEL](https://python.langchain.com/docs/expression_language/)\n",
        "\n",
        "## Explore base\n",
        "- [The Langchain Interface: Chains and Runnables](https://jordan-mungujakisa.medium.com/the-langchain-interface-chains-and-runnables-cd2f2cb6b4d6#:~:text=To%20make%20it%20easy%20to,chain%20on%20a%20single%20input)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
